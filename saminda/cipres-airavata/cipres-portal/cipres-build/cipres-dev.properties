tool.registry.home=cipres

tool.resource.cluster=TSCC
tscc.login=tscc
tscc.fileHost=tscc
tscc.submit=submit_v2.py
tscc.check=checkjobs_v2.py
tscc.cancel=delete_v2.py
tscc.rc=/home/cipres_test/ngbw/contrib/scripts/workbench.rc
tscc.project=cipres_test
tscc.workspace=/projects/ps-ngbt/backend/tscc_test_workspace

tool.resource.host=billiards	
host=billiards
# directory must be owned by "bwbatch":  See sdk/resources/ssl.properties use
# of billiards.username and billiards.password and their definitions in ~/.m2.settings.xml.
workspace=/users/u4/bwbatch/workspace

teragrid.trestles.workspace=/projects/ps-ngbt/backend/trestles_test_workspace
teragrid.trestles.host=trestles-dm1.sdsc.edu
teragrid.trestles.user=
teragrid.trestles.login=cipres@trestles.sdsc.edu
teragrid.trestles.rc=/projects/ps-ngbt/home/cipres/.bash_profile
teragrid.trestles.submit=submit_v2.py
teragrid.trestles.check=checkjobs_v2.py
teragrid.trestles.cancel=delete_v2.py
trestles.filehandler=org.ngbw.sdk.tool.GridFtpFileHandler

teragrid.gordon.workspace=/projects/ps-ngbt/backend/gordon_test_workspace
teragrid.gordon.host=trestles-dm1.sdsc.edu
teragrid.gordon.user=
teragrid.gordon.login=cipres@gordon.sdsc.edu
teragrid.gordon.rc=/projects/ps-ngbt/home/cipres/.bash_profile
teragrid.gordon.submit=gordon_submit_v2.py
teragrid.gordon.check=checkjobs_v2.py
teragrid.gordon.cancel=delete_v2.py
gordon.filehandler=org.ngbw.sdk.tool.GridFtpFileHandler

teragrid.tools.disable=false

# ####################################################################################
# GLOBUS  Properties 
# ####################################################################################

# Units for the following are in minutes (1 hr=60, 1 day = 1440, 3 days = 4320, 7 days = 10080, etc)
grid.job.lifetime.max = 10080 

# Globus process worker polls very frequently the first minute, then at quick.poll.interval for 
# several minutes, then medium poll interval  and finally at poll.interval.
grid.job.poll.interval = 10 
grid.job.medium.poll.interval = 1 
grid.job.quick.poll.interval = 1 

# Lifetime of credential in hours.  min.lifetime  must be less than lifetime.
# With the credential we're using (a default teragrid credential) 7 days = 168 hrs is the max
# we can get. 
grid.myproxy.lifetime = 168 
grid.myproxy.min.lifetime = 144 

# ####################################################################################
# END GLOBUS  Properties 
# ####################################################################################

# Used for sending job completion email 
# Properties are used in: /src/main/resources/tool/[job-html.ftl, job-text.ftl, spring-mail.xml]
portal.name=Cipres REST Dev
job.email.from=Cipres Web Portal <customerservice@phylo.org>
job.email.enable=true

# maxPoolSize needs to be at least ( 2 * max(loadResults.pool.size, submitJob.pool.size) ) + 1
# to avoid deadlock.  Also make sure that in ssl.properties, maxconn for the hosts we're using
# is at least  2 * max(loadResults.pool.size, submitJob.pool.size) 
database.maxPoolSize=10
database.minPoolSize=1

ssl.maxconn=8

# test and dev database for rest development, use on billiards. 
# database.url=jdbc:log4jdbc:mysql://mysql.sdsc.edu:3312/ngbwalpha
# database.fileRoot=/archive/science/ngbt/db_documents/ngbw_alpha
#database.url=jdbc:log4jdbc:mysql://mysql.sdsc.edu:3316/cipres_copy
#database.fileRoot=/archive/science/ngbt/db_documents/ngbw_test

# Terri's local mac database for rest development
# database.url=jdbc:log4jdbc:mysql://mysql.sdsc.edu:3312/ngbwalpha
# database.fileRoot=/Users/terri/db_documents/ngbw_alpha

# Poll remote host for completed jobs every 1 minutes
loadResults.poll.seconds=10
loadResults.pool.size=3
loadResults.retries=3

submitJob.poll.seconds=10
submitJob.pool.size=3
submitJob.retries=3

checkJobs.poll.seconds=60
checkjobs.maxDays = 30

accounting.period.start=2013-07-01
iplant.charge.number=TG-MCB110022

# START: These are USER and MACHINE specific.  Please be sure to change them!
# DON'T USE "localhost" in portal.url or your installation will interfere with
# other installations jobs. 
#   
grid.myproxyfile=/users/u4/terri/cipres_data/x509_cipres
disabled.resources.file=/mnt/dev/testbed/cipres_data/disabled_resources
logs=/mnt/dev/testbed/cipres_data

portal.url=http://somerandom.url/portal2/
portal.callback.url=http://somerandom.url/portal2/taskupdate.action
rest.callback.url=-K ~/.jobcurl.rc -k https://somerandom.url/cipresrest/v1/job/admin/updateJob
server_url=https://billiards1.sdsc.edu/cipresrest/v1/job
rest_url=https://billiards1.sdsc.edu/cipresrest/v1
database.url=jdbc:log4jdbc:mysql://mysql.sdsc.edu:3316/cipres_copy
database.fileRoot=/mnt/dev/testbed/cipres_data/db_documents
# appkey=1
# appname=guitest
# umbrella_username=guitest_admin
# umbrella_password=test
# pycipres_appkey=1
# pycipres_appname=scripts

# Terri's local mac version of machine specific stuff:
# grid.myproxyfile=/Users/terri/cipres_data/x509_cipres
# disabled.resources.file=/Users/terri/cipres_data/disabled_resources
# portal.url=http://somehost/cipresrest/
# job.callback.url=-K ~/.jobcurl.rc http://somehost/cipresrest/v1/job/admin/updateJob

# END: These are USER and MACHINE specific.  

